{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "\n",
    "from keys import API_KEY\n",
    "\n",
    "import os \n",
    "os.environ['OPENAI_API_KEY'] = API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [Path().cwd().joinpath('papers').joinpath(paper) for paper in os.listdir('papers')]\n",
    "path = str(paths[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "question = \"What approach is the author using?\"\n",
    "docs = vectorstore.similarity_search(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What approach is the author using?\"\n",
    "docs = vectorstore.similarity_search(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What approach is the author using?',\n",
       " 'result': 'The author is comparing and evaluating two different approaches: the constant volatility scaling (CVS) approach and the dynamic volatility scaling (DVS) approach.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectorstore.as_retriever())\n",
    "qa_chain({\"query\": question})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Summarize Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "loader = PyPDFLoader(path)\n",
    "pages = loader.load()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = chain.run(pages)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = 'The study \"Risk adjusted momentum strategies: a comparison between constant and dynamic volatility scaling approaches\" by Fan, Li, and Liu (2018) investigates the efficacy of two volatility scaling methods in momentum strategies: the constant volatility scaling (CVS) approach of Barroso and Santa-Clara (2015), and the dynamic volatility scaling (DVS) method of Daniel and Moskowitz (2016). These methods are designed to reduce the risks associated with momentum strategies, which, while capable of generating consistent abnormal returns, are also susceptible to significant crashes. \\n\\nThe researchers applied these strategies to a diversified portfolio of 55 global liquid futures contracts, spanning from June 1986 to May 2017. The CVS-based momentum strategy was found to be the most efficient, yielding an annual return of 15.3%. The study further segments the sample period into three sub-periods: pre-crisis, crisis, and post-crisis for a comprehensive analysis. The CVS-based strategy outperformed the DVS-based strategy in the pre-crisis period, but this superiority became statistically insignificant during and after the crisis. \\n\\nThe study also examines the impact of ex-ante realized volatility and bear market indicators on WML return using the method of Daniel and Moskowitz (2016). They found that high volatility lowers market expectation and vice versa. The volatility scaled strategies outperformed unscaled strategies, with the CVS-based strategy remaining the most profitable among all. \\n\\nThe study concludes that the CVS approach is a more efficient volatility scaling method for momentum strategies in futures markets, despite incorporating relatively larger risk and drawdown. It ends up with the highest cumulative returns, outperforming all other strategies including the equally weighted buy-and-hold, TSMOM and XSMOM as well as scaled buy-and-hold and TSMOM strategies. However, the study also acknowledges that the CVS approach displays higher risks compared to other volatility scaling approaches, which might affect its profitability in times of uncertainty. The researchers suggest that future research could focus on investigating the source of this risk and how to alleviate it. \\n\\nThe study, published in Research in International Business and Finance, is openly accessible via the Queen\\'s University Belfast Research Portal. The authors retain copyright, but the work is available under the CC-BY-NC-ND 4.0 license, allowing noncommercial distribution and reproduction with proper citation.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'summariessummary__{paths[0].name.split(\".pdf\")[0]}.tex', 'w') as file:\n",
    "    file.write(latex_document_str(summary))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuff Summarise Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The study compares the performance of two volatility scaling methods, constant volatility scaling (CVS) and dynamic volatility scaling (DVS), in momentum strategies. The researchers perform momentum strategies based on these two approaches in a diversified portfolio consisting of 55 global liquid futures contracts. They also compare the results to time series momentum and buy-and-hold strategies. \n",
      "\n",
      "The findings show that the momentum strategy based on the constant volatility scaling method (CVS) is the most efficient approach with an annual return of 15.3%. The CVS approach outperforms the DVS approach in terms of alpha, or excess return, with a statistically significant difference. \n",
      "\n",
      "The study also identifies a momentum crash in futures markets during the 2009-2013 period, which is attributed to the 2007-2008 global financial crisis. \n",
      "\n",
      "Overall, the study concludes that the CVS approach is more efficient and profitable than the DVS approach in momentum strategies. However, the superiority of the CVS approach is reduced during times of financial crisis.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "# Define prompt USE THIS TO CUSTOMISE!!!\n",
    "prompt_template = \"\"\"Write a detailed summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Define LLM chain\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain, document_variable_name=\"text\"\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(stuff_chain.run(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Reduce Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes \n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{doc_summaries}\n",
    "Take these and distill it into a final, consolidated summary of the main themes. \n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not perform as good as the normal stuff summarisation (but could be very useful for other areas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refine chain is the best option (But expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "loader = PyPDFLoader(path)\n",
    "pages = loader.load()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "\n",
    "prompt_template = \"\"\"Write a detailed summary of the following:\n",
    "{text}\\n\n",
    "DETAILED SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "refine_template = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=False,\n",
    "    input_key=\"input_documents\",\n",
    "    output_key=\"output_text\",\n",
    ")\n",
    "result = chain({\"input_documents\": pages}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.tex', 'w') as file:\n",
    "    file.write(latex_document_str('Test Header', result['output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\documentclass{article}\n",
      "\\title{Test Header}\n",
      "\\begin{document} \n",
      "\n",
      "\\maketitle\n",
      "\n",
      "The research paper \"Risk adjusted momentum strategies: a comparison between constant and dynamic volatility scaling approaches\" by Fan, Li, and Liu (2018) explores the effectiveness of risk-adjusted momentum strategies using two different volatility scaling approaches: constant and dynamic. The study uses data from the US stock market and a diversified portfolio of 55 global liquid futures contracts, including commodities, sovereign bonds, currencies, and equity index contracts from various exchanges such as COMEX and TOCOM.\\\\\n",
      "\\\\\n",
      "The constant volatility scaling approach assumes that the volatility of the stock returns is constant over time, while the dynamic approach allows for changes in volatility over time. The study finds that the dynamic volatility scaling approach outperforms the constant volatility scaling approach in terms of risk-adjusted returns, as it provides a more accurate measure of risk, leading to better portfolio performance. \\\\\n",
      "\\\\\n",
      "However, the study also reveals that the constant volatility scaling approach yields significantly higher abnormal returns than the dynamic approach during certain periods, such as before the 2007-2008 financial crisis. The superiority of the constant approach becomes statistically insignificant during and after the crisis. \\\\\n",
      "\\\\\n",
      "The study also compares these strategies with time series momentum and buy-and-hold strategies. The results show that the momentum strategy based on the constant volatility scaling method is the most efficient approach with an annual return of 15.3\\%. The authors also discuss the risks associated with momentum strategies, including occasional large crashes, and how volatility scaling methods can mitigate these risks. \\\\\n",
      "\\\\\n",
      "The authors conclude that their findings offer valuable insights for investors and portfolio managers in designing and implementing risk-adjusted momentum strategies. They suggest that future research should further explore the potential benefits of the dynamic volatility scaling approach in other financial markets and asset classes. The paper also identifies the momentum crash in futures markets, demonstrating the reasonableness to employ the volatility scaling approaches. \\\\\n",
      "\\\\\n",
      "The study further divides the entire sample period into three sub-periods: pre-crisis, crisis, and post-crisis, and regresses the returns of CVS and DVS based XSMOM strategies on four market indices proxying different asset classes as well as the Fama-French and Carhart three factor models representing size, value, and momentum effects. The regression results show that both CVS and DVS based XSMOM strategies display statistically significant alphas in the pre-crisis period. \\\\\n",
      "\\\\\n",
      "The study also reveals that the CVS based XSMOM strategy outperforms the DVS based XSMOM strategy as the difference in alphas is statistically significant at the 1\\% level during the overall period and the pre-crisis period. However, the difference becomes insignificant during the crisis and post-crisis periods. The study also observes that the cumulative returns of the CVS based XSMOM strategy are slightly higher than the DVS based XSMOM strategy before 2003, but the superiority expands between 2003 and 2007. The recession during the 2007-2008 financial crisis nearly eliminates the difference between the two scaled strategies. After the financial crisis, the recovery of global economic condition improves the performance of both scaled strategies with the gap being quite small. \\\\\n",
      "\\\\\n",
      "The study concludes that the CSV is a more efficient volatility scaling method for momentum strategies in futures markets, despite incorporating relatively larger risk and drawdown. It ends up with the highest cumulative returns, significantly outperforming the benchmark strategies. The authors suggest that the findings provide valuable insights for investors and portfolio managers in designing and implementing risk-adjusted momentum strategies. They also recommend that future research should further explore the potential benefits of the dynamic volatility scaling approach in other financial markets and asset classes.\\\\\n",
      "\\\\\n",
      "One of the main concerns for users of the CVS approach is that it displays higher risks (standard deviation) compared to the other volatility scaling approaches which might affect its profitability in times of uncertainty. The authors suggest future researches could be focused on investigating the source of this risk and how to alleviate it. One possible method is to rank the winner/loser portfolio using an alternative way, instead of ranking their returns. \n",
      "\n",
      "\\end{document}\n"
     ]
    }
   ],
   "source": [
    "print(latex_document_str('Test Header', result['output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import cached_property\n",
    "\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def latex_document_str(header, main_text):\n",
    "    main_text = main_text.replace(\"\\n\\n\", \" \\\\\\\\\\n\\\\\\\\\\n\").replace('%', r'\\%')\n",
    "    string = (\n",
    "        r\"\\documentclass{article}\"+\"\\n\"\n",
    "        r\"\\title{\"+header+\"}\"+\"\\n\"\n",
    "        r\"\\begin{document} \"+\"\\n\\n\"\n",
    "        r\"\\maketitle\"+\"\\n\\n\"\n",
    "        r\"\"+main_text+\" \\n\\n\"\n",
    "        r\"\\end{document}\"\n",
    "    )\n",
    "    return string    \n",
    "\n",
    "@dataclass\n",
    "class PdfLLM:\n",
    "    filepath: str\n",
    "\n",
    "    def __post_init__(self):\n",
    "\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "\n",
    "        self.detailed_prompt_template = (\n",
    "            \"Write a detailed summary of the following:\\n\"\n",
    "            \"{text}\\n\\n\"\n",
    "            \"DETAILED SUMMARY:\")\n",
    "\n",
    "        self.concise_prompt_template = (\n",
    "            \"Write a concise summary of the following:\\n\"\n",
    "            \"{text}\\n\\n\"\n",
    "            \"CONCISE SUMMARY:\")\n",
    "        \n",
    "        self.refine_template = (\n",
    "            \"Your job is to produce a final summary\\n\"\n",
    "            \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "            \"We have the opportunity to refine the existing summary\"\n",
    "            \"(only if needed) with some more context below.\\n\"\n",
    "            \"------------\\n\"\n",
    "            \"{text}\\n\"\n",
    "            \"------------\\n\"\n",
    "            \"Given the new context, refine the original summary.\"\n",
    "            \"If the context isn't useful, return the original summary.\"\n",
    "        )\n",
    "\n",
    "    def save_latex_summary(self):\n",
    "        pass\n",
    "\n",
    "    def ask_question(self, question):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size = 1000,\n",
    "            chunk_overlap = 50\n",
    "        )\n",
    "        splits = text_splitter.split_documents(self.pages)\n",
    "        vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "        llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "        qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectorstore.as_retriever())\n",
    "        return qa_chain({\"query\": question})['result']\n",
    "\n",
    "    @cached_property\n",
    "    def concise_summary(self):\n",
    "        return self.get_summary(self.concise_prompt_template)\n",
    "    \n",
    "    @cached_property\n",
    "    def detailed_summary(self):\n",
    "        return self.get_summary(self.detailed_prompt_template)\n",
    "\n",
    "    def get_summary(self, promt_template):\n",
    "        chain = self.get_chain(promt_template)\n",
    "        result = chain({\"input_documents\": self.pages}, return_only_outputs=True)\n",
    "        return result['output_text']\n",
    "    \n",
    "    @cached_property\n",
    "    def pages(self):\n",
    "        loader = PyPDFLoader(self.filepath)\n",
    "        pages = loader.load()\n",
    "        return pages \n",
    "\n",
    "    def get_chain(self, promt_template):\n",
    "        prompt = PromptTemplate.from_template(prompt_template)\n",
    "        refine_prompt = PromptTemplate.from_template(self.refine_template)\n",
    "        chain = load_summarize_chain(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"refine\",\n",
    "            question_prompt=prompt,\n",
    "            refine_prompt=refine_prompt,\n",
    "            return_intermediate_steps=False,\n",
    "            input_key=\"input_documents\",\n",
    "            output_key=\"output_text\",\n",
    "        )\n",
    "        return chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfllm = PdfLLM(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bloomberg Ticker Sector Start End Mean SD'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfllm.ask_question('What is the header? Return only the header:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import InvalidRequestError\n",
    "from keys import API_KEY\n",
    "import PyPDF2\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import fitz\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "\n",
    "@lru_cache\n",
    "def query_gpt(text, model='gpt-3.5-turbo', **kwargs):\n",
    "\n",
    "    import openai\n",
    "    openai.api_key = API_KEY\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful research assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        model=model,\n",
    "        **kwargs\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def query_gpt_with_context_concise(text, context, model='gpt-3.5-turbo', **kwargs):\n",
    "    import openai\n",
    "    openai.api_key = API_KEY\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"Context: {context} \\n\\n You are a helpful research assistant. Please answer as concisely as possible and use the context to inform the answer. If the context does not have enough information, reply with 'NA'\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        model=model,\n",
    "        **kwargs\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "def query_gpt_with_context(text, context, model='gpt-3.5-turbo', **kwargs):\n",
    "\n",
    "    import openai\n",
    "    openai.api_key = API_KEY\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"Context: {context} \\n\\n You are a helpful research assistant. Please use the context to inform the answer. If the context does not have enough information, reply with 'NA'\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        model=model,\n",
    "        **kwargs\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "@lru_cache\n",
    "def query_gpt_with_context_detailed(text, context, model='gpt-3.5-turbo', **kwargs):\n",
    "\n",
    "    import openai\n",
    "    openai.api_key = API_KEY\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"Context: {context} \\n\\n You are a helpful research assistant. Please answer detailed and use the context to inform the answer. If the context does not have enough information, reply with 'NA'\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        model=model,\n",
    "        **kwargs\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "\n",
    "def get_summary(text):\n",
    "\n",
    "    import openai\n",
    "    openai.api_key = API_KEY\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful research assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Summarize this: {text}\"},\n",
    "        ],\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "\n",
    "def flags_decomposer(flags):\n",
    "    \"\"\"Make font flags human readable.\"\"\"\n",
    "    l = []\n",
    "    if flags & 2 ** 0:\n",
    "        l.append(\"superscript\")\n",
    "    if flags & 2 ** 1:\n",
    "        l.append(\"italic\")\n",
    "    if flags & 2 ** 2:\n",
    "        l.append(\"serifed\")\n",
    "    else:\n",
    "        l.append(\"sans\")\n",
    "    if flags & 2 ** 3:\n",
    "        l.append(\"monospaced\")\n",
    "    else:\n",
    "        l.append(\"proportional\")\n",
    "    if flags & 2 ** 4:\n",
    "        l.append(\"bold\")\n",
    "    return \", \".join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summaries(text, chapters):\n",
    "\n",
    "    text_by_chapter = {}\n",
    "    summary_by_chapter = {}\n",
    "    statistical_tools = {}\n",
    "    data = {}\n",
    "    for chapter, next_chapter in tqdm(zip(chapters, chapters[1:] + ['end'])):\n",
    "        chapter_loc = text.lower().find(chapter.lower())\n",
    "        next_chapter_loc = text[chapter_loc:].lower().find(next_chapter.lower()) + chapter_loc\n",
    "        text_by_chapter[chapter] = text[chapter_loc:next_chapter_loc]\n",
    "        data[chapter] = [chapter_loc, next_chapter_loc]\n",
    "        try: \n",
    "            summary_by_chapter[chapter] = query_gpt(f'Summarise this chapter of a research paper called \"{chapter}\": {text_by_chapter[chapter]}')\n",
    "            # statistical_tools[chapter] = query_gpt(f'What statistical tools are the authors using in this chapter? Return just a list of tools. No text, just list. Use \"*\" as bullet points: {text_by_chapter[chapter]}')\n",
    "        except InvalidRequestError as e:\n",
    "            e = str(e)\n",
    "            num_location = e.find('in ')\n",
    "            num = int(e[num_location+3: num_location+8])\n",
    "            num_calls = math.ceil((num/4000))\n",
    "            split_text = [text_by_chapter[chapter][i*4000: (i+1)*4000] for i in range(num_calls)]\n",
    "            summary_by_chapter[chapter] = ' '.join([query_gpt(f'Summarise this part of a chapter of a research paper: {text}') for text in split_text])\n",
    "            # statistical_tools[chapter] = ' '.join([query_gpt(f'What statistical tools are the authors using in this part? Return just a list of tools. No text, just list. Use \"*\" as bullet points: {text}') for text in split_text])\n",
    "\n",
    "    return summary_by_chapter, statistical_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict, List\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from functools import lru_cache\n",
    "import os\n",
    "\n",
    "class GPTDetail(str, Enum):\n",
    "    concise = 'concise'\n",
    "    detailed = 'detailed'\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class PDFAnalyser:\n",
    "    file_path: str = None\n",
    "    logger = logging.getLogger('log')\n",
    "\n",
    "    def load_pdf_detailed(self) -> pd.DataFrame:\n",
    "\n",
    "        pdf_details_list = []\n",
    "        with fitz.open(self.file_path) as doc:\n",
    "\n",
    "            for page_no, page in enumerate(doc.pages()):\n",
    "                blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "\n",
    "                for block in blocks: \n",
    "                    block_no = block['number']\n",
    "                    block_str = ''\n",
    "                    block_bold = []\n",
    "                    block_italic = []\n",
    "                    if 'lines' in block.keys():\n",
    "                        for line in block['lines']:\n",
    "                            for span in line['spans']:\n",
    "                                block_str += span['text']\n",
    "                                flags = flags_decomposer(span['flags'])\n",
    "                                block_bold.append('bold' in flags)\n",
    "                                block_italic.append('italic' in flags)\n",
    "                                location = span['origin'][0]\n",
    "                                size = span['size']\n",
    "\n",
    "                    \n",
    "                    # add block to pdf_details if block has text\n",
    "                    if len(block_str.strip()) > 0:\n",
    "                        pdf_details_list.append([page_no, block_no ,block_str, all(block_bold), all(block_italic), location, size])\n",
    "\n",
    "        pdf_details = pd.DataFrame(pdf_details_list, columns=['page', 'block', 'text', 'bold', 'italic', 'location', 'size']).set_index(['page', 'block'])\n",
    "        pdf_details['formatting'] = pdf_details['bold'] | pdf_details['italic']\n",
    "        pdf_details['text'] = pdf_details['text'].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii')) #remove weird encoding\n",
    "\n",
    "        self._full_text = ' '.join(list(pdf_details['text']))\n",
    "        self._pages = {page: ' '.join(list(pdf_details.loc[pdf_details.index.get_level_values(0)==page, 'text'])) for page in pdf_details.index.get_level_values(0).unique()}\n",
    "        \n",
    "        return pdf_details\n",
    "    \n",
    "    def create_toc(self, pdf_details:pd.DataFrame, formatting:str='bold', variance_minimum:float=5, remove_keywords:List=['exhibit', 'table']) -> pd.DataFrame:\n",
    "        \n",
    "        # get unique locations of blocks and number of ocurrences \n",
    "        locations = pdf_details.loc[pdf_details[formatting]].groupby('location').count()['text'].sort_values()\n",
    "        \n",
    "        # go through each location and determine if it should be part of TOC (part of location, formatting, variance of sentence length within group)\n",
    "        toc_options = pd.DataFrame()\n",
    "        for i in range(1, len(locations)):    \n",
    "            n_most_location_bold = pdf_details.loc[(pdf_details.location.round(2) == round(locations.index[-i], 2)) & pdf_details[formatting]].copy()\n",
    "            n_most_location_bold['variance'] = n_most_location_bold['text'].apply(lambda x: len(x)).var()\n",
    "            toc_options = pd.concat([toc_options, n_most_location_bold])\n",
    "        \n",
    "        if len(toc_options.index) == 0:\n",
    "            raise ValueError('TOC could not be generated, try a different pdf')\n",
    "\n",
    "        \n",
    "        toc = toc_options.loc[\n",
    "            (toc_options.variance > variance_minimum) \n",
    "            & ~(toc_options.variance.isna())].drop_duplicates().sort_index() #remove those that are not relevant\n",
    "        \n",
    "        # remove keywords that don't belong in TOC\n",
    "        for keyword in remove_keywords:\n",
    "            toc = toc.loc[toc.text.apply(lambda x: keyword not in x.replace(' ', '').lower())].copy()\n",
    "        \n",
    "        if len(toc.index) == 0:\n",
    "            raise ValueError('TOC could not be generated, try a different pdf')\n",
    "\n",
    "        # If the word intro does not exist, set the start of the second page as first chapter\n",
    "        if 'intro' not in toc.iloc[0].at['text'].lower():\n",
    "            toc.loc[(-1,0), 'text'] = pdf_details.loc[(1,)].iloc[0].at['text']\n",
    "        \n",
    "        toc = toc.sort_index()\n",
    "        toc = list(toc['text'])\n",
    "\n",
    "        # try to remove everything after conclusion/concluding remarks etc.\n",
    "        try:\n",
    "            toc = toc[:toc.index([i for i in toc if 'concl' in i.lower()][0])+1]\n",
    "        except Exception as e:\n",
    "            print(f'toc has no conclusion: {e}')\n",
    "\n",
    "        concl_loc = pdf_details.index.get_loc(tuple(pdf_details.loc[pdf_details.text == toc[-1]].index)[0])\n",
    "        post_conc_loc = pdf_details.index.get_loc(tuple(pdf_details.iloc[concl_loc+1:].loc[pdf_details.bold].iloc[0].name))\n",
    "        toc.append(pdf_details.iloc[post_conc_loc].loc['text'])\n",
    "\n",
    "        return toc\n",
    "    \n",
    "    def get_chapter_text(self, text, toc) -> dict:\n",
    "\n",
    "        next_chapter_loc = 0\n",
    "        text_by_chapter = {}\n",
    "\n",
    "        for chapter, next_chapter in tqdm(zip(toc[:-1], toc[1:])):\n",
    "            chapter_loc = text[next_chapter_loc:].lower().find(chapter.lower()) + next_chapter_loc\n",
    "            next_chapter_loc = text[chapter_loc:].lower().find(next_chapter.lower()) + chapter_loc\n",
    "            chapter_text = text[chapter_loc:next_chapter_loc]\n",
    "            text_by_chapter[chapter] = chapter_text\n",
    "        \n",
    "        return text_by_chapter \n",
    "    \n",
    "    def summarise_parts(self, parts:dict) -> str:\n",
    "        summaries = {}\n",
    "        for part_name, part in (pbar := tqdm(parts.items())):\n",
    "            pbar.set_description(f'Summarising {part_name}: ')\n",
    "            summaries[part_name] = self.summarise_text(part)\n",
    "        return summaries\n",
    "\n",
    "    def summarise_text(self, text:str) -> str:\n",
    "        \n",
    "        try:\n",
    "            # print(text+'\\n\\n\\n')\n",
    "            summary = query_gpt(f'Summarise this: {text}')\n",
    "        except InvalidRequestError as e:\n",
    "            max_length, current_length = (int(s) for s in str(e).split() if s.isdigit())\n",
    "            ratio = current_length / max_length\n",
    "            if ratio % 1 < 0.8: num_parts = int(ratio) + 1\n",
    "            else: num_parts = int(ratio) + 2\n",
    "            part_length = int(len(text) / num_parts ) + 1\n",
    "            split_text = [text[i*part_length: i+1*part_length] for i in range(num_parts)]\n",
    "            summary = '\\n'.join([query_gpt(f'Summarise this part of a chapter of a research paper: {text}') for text in split_text])\n",
    "        return summary\n",
    "\n",
    "    def run_page_summary(self) -> str:\n",
    "        return self.prettify_part_summary(self.summarise_parts(self.pages))        \n",
    "\n",
    "    def run_chapter_summary(self) -> str:\n",
    "        return self.prettify_part_summary(self.summarise_parts(self.chapters))        \n",
    "\n",
    "    def prettify_part_summary(self, summaries:dict):\n",
    "        return '\\n'.join([str(part_name)+'\\n'+'='*len(str(part_name))+'\\n'+part+'\\n\\n' for part_name, part in summaries.items()])\n",
    "\n",
    "    @property\n",
    "    def full_text(self) -> str:\n",
    "        if not hasattr(self, '_full_text'):\n",
    "            self.load_pdf_detailed()\n",
    "        return self._full_text\n",
    "\n",
    "    @property\n",
    "    def pages(self) -> dict:\n",
    "        if not hasattr(self, '_pages'):\n",
    "            self.load_pdf_detailed()\n",
    "        return self._pages\n",
    "    \n",
    "    @property\n",
    "    def toc(self) -> list:\n",
    "        if not hasattr(self, '_toc'):\n",
    "            pdf_details = self.load_pdf_detailed()\n",
    "            self._toc = self.create_toc(pdf_details)\n",
    "        return self._toc\n",
    "    \n",
    "    @property\n",
    "    def chapters(self) -> dict:\n",
    "        if not hasattr(self, '_chapters'):\n",
    "            self._chapters = self.get_chapter_text(self.full_text, self.toc)\n",
    "        return self._chapters\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparison_between_constant_and_dynamic_vol_scaling \n",
      " ===================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00, 20956.55it/s]\n",
      "Summarising 5. Conclusion: : 100%|██████████| 7/7 [00:00<00:00, 13.99it/s]                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "betting_against_beta \n",
      " ====================\n",
      "TOC could not be generated, try a different pdf \n",
      "\n",
      "\n",
      "\n",
      "short_and_long_horizon_behavioural \n",
      " ==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00, 6499.92it/s]\n",
      "Summarising 6. Conclusion: : 100%|██████████| 7/7 [00:01<00:00,  6.07it/s]                                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "qual_min_junk \n",
      " =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00, 7909.52it/s]\n",
      "Summarising 6. Conclusion : : 100%|██████████| 7/7 [00:00<00:00, 25.66it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "a_five_factor_asset_pricing_model \n",
      " =================================\n",
      "TOC could not be generated, try a different pdf \n",
      "\n",
      "\n",
      "\n",
      "momentum_and_the_cross_section_of_volatility \n",
      " ============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00, 12033.39it/s]\n",
      "Summarising 7. Conclusion: : 100%|██████████| 8/8 [00:00<00:00, 11.78it/s]                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "factor_momentum_and_momentum_factor \n",
      " ===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:00, 12037.38it/s]\n",
      "Summarising 6Conclusion: : 100%|██████████| 25/25 [01:39<00:00,  3.98s/it]                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "my_factor_phil \n",
      " ==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 6142.80it/s]\n",
      "Summarising VI. Conclusion : : 100%|██████████| 5/5 [00:29<00:00,  5.96s/it]                                                                                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "momentum_has_its_moments \n",
      " ========================\n",
      "TOC could not be generated, try a different pdf \n",
      "\n",
      "\n",
      "\n",
      "stock_return_predictability_is_it_there \n",
      " =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:00, 6555.58it/s]\n",
      "Summarising 6Bias, Size and Power: :  68%|██████▊   | 21/31 [02:23<01:08,  6.85s/it]                                            \n"
     ]
    },
    {
     "ename": "ServiceUnavailableError",
     "evalue": "The server is overloaded or not ready yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServiceUnavailableError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     analyser \u001b[39m=\u001b[39m PDFAnalyser(path)\n\u001b[0;32m----> 8\u001b[0m     chapter_summary \u001b[39m=\u001b[39m analyser\u001b[39m.\u001b[39;49mrun_chapter_summary()\n\u001b[1;32m      9\u001b[0m     structured_summary \u001b[39m=\u001b[39m query_gpt_with_context_detailed(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWrite a detailed summary of the context with this structure: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m1. Introduction \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m2. Data & Methodology \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m3. Results.\u001b[39m\u001b[39m'\u001b[39m, chapter_summary)\n\u001b[1;32m     10\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39moutputs/\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m_summary.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n",
      "Cell \u001b[0;32mIn[7], line 140\u001b[0m, in \u001b[0;36mPDFAnalyser.run_chapter_summary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_chapter_summary\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprettify_part_summary(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msummarise_parts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchapters))\n",
      "Cell \u001b[0;32mIn[7], line 118\u001b[0m, in \u001b[0;36mPDFAnalyser.summarise_parts\u001b[0;34m(self, parts)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mfor\u001b[39;00m part_name, part \u001b[39min\u001b[39;00m (pbar \u001b[39m:=\u001b[39m tqdm(parts\u001b[39m.\u001b[39mitems())):\n\u001b[1;32m    117\u001b[0m     pbar\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSummarising \u001b[39m\u001b[39m{\u001b[39;00mpart_name\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m     summaries[part_name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msummarise_text(part)\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m summaries\n",
      "Cell \u001b[0;32mIn[7], line 125\u001b[0m, in \u001b[0;36mPDFAnalyser.summarise_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msummarise_text\u001b[39m(\u001b[39mself\u001b[39m, text:\u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    123\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m         \u001b[39m# print(text+'\\n\\n\\n')\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m         summary \u001b[39m=\u001b[39m query_gpt(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mSummarise this: \u001b[39;49m\u001b[39m{\u001b[39;49;00mtext\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    126\u001b[0m     \u001b[39mexcept\u001b[39;00m InvalidRequestError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m         max_length, current_length \u001b[39m=\u001b[39m (\u001b[39mint\u001b[39m(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e)\u001b[39m.\u001b[39msplit() \u001b[39mif\u001b[39;00m s\u001b[39m.\u001b[39misdigit())\n",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m, in \u001b[0;36mquery_gpt\u001b[0;34m(text, model, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenai\u001b[39;00m\n\u001b[1;32m      9\u001b[0m openai\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m API_KEY\n\u001b[0;32m---> 11\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     12\u001b[0m     messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     13\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mYou are a helpful research assistant.\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m     14\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: text},\n\u001b[1;32m     15\u001b[0m     ],\n\u001b[1;32m     16\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     17\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/coding/gpt_research/gpt_venv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/Desktop/coding/gpt_research/gpt_venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/Desktop/coding/gpt_research/gpt_venv/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/Desktop/coding/gpt_research/gpt_venv/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/coding/gpt_research/gpt_venv/lib/python3.10/site-packages/openai/api_requestor.py:743\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[39mreturn\u001b[39;00m OpenAIResponse(\u001b[39mNone\u001b[39;00m, rheaders)\n\u001b[1;32m    742\u001b[0m \u001b[39mif\u001b[39;00m rcode \u001b[39m==\u001b[39m \u001b[39m503\u001b[39m:\n\u001b[0;32m--> 743\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mServiceUnavailableError(\n\u001b[1;32m    744\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe server is overloaded or not ready yet.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    745\u001b[0m         rbody,\n\u001b[1;32m    746\u001b[0m         rcode,\n\u001b[1;32m    747\u001b[0m         headers\u001b[39m=\u001b[39mrheaders,\n\u001b[1;32m    748\u001b[0m     )\n\u001b[1;32m    749\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    750\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtext/plain\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m rheaders\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[0;31mServiceUnavailableError\u001b[0m: The server is overloaded or not ready yet."
     ]
    }
   ],
   "source": [
    "paths = [Path().cwd().joinpath('papers').joinpath(paper) for paper in os.listdir('papers')]\n",
    "Path('outputs').mkdir(exist_ok=True)\n",
    "for path in paths:\n",
    "    name = path.stem\n",
    "    print(name, '\\n', len(name)*'=')\n",
    "    try:\n",
    "        analyser = PDFAnalyser(path)\n",
    "        chapter_summary = analyser.run_chapter_summary()\n",
    "        structured_summary = query_gpt_with_context_detailed(f'Write a detailed summary of the context with this structure: \\n\\n1. Introduction \\n2. Data & Methodology \\n3. Results.', chapter_summary)\n",
    "        with open(f'outputs/{name}_summary.txt', 'w') as file:\n",
    "            file.writelines(structured_summary)\n",
    "    except ValueError as e:\n",
    "        print(e, '\\n')\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overview(summary):\n",
    "    full_text = ''\n",
    "    for key, value in summary.items():\n",
    "        full_text += key.upper()+'\\n'\n",
    "        full_text += '='*len(key)+'\\n'\n",
    "        full_text += value+'\\n\\n'\n",
    "\n",
    "    overview_string = ''\n",
    "    overview_string += 'HYPOTHESIS\\n'+query_gpt(f'What is the authors hypothesis?: \\n{full_text}')+'\\n\\n'\n",
    "    overview_string += 'DATE RANGE\\n'+query_gpt(f'What is the date range used in this study? : \\n{full_text}')+'\\n\\n'\n",
    "    overview_string += 'UNIVERSE\\n'+query_gpt(f'What country are the authors analysing? Answer with just the country: \\n{full_text}')+'\\n\\n'\n",
    "    overview_string += 'CONCLUSION\\n'+query_gpt(f'What is the authors conclusion?: \\n{full_text}')+'\\n\\n'\n",
    "        \n",
    "    return full_text, overview_string\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_references(pdf_details, words_to_try='bold'):\n",
    "    for word in words_to_try:\n",
    "        try:\n",
    "            page_num = tuple(pdf_details.loc[(pdf_details.text.apply(lambda x: word in x.lower()))].index[0])\n",
    "            pdf_details.loc[pdf_details.index.get_level_values(0) >= page_num[0]]\n",
    "            references_until_end = pdf_details.iloc[pdf_details.index.get_loc(page_num)+1:]\n",
    "            try:\n",
    "                end_of_references = references_until_end.index.get_loc(references_until_end.loc[references_until_end.formatting].iloc[0].name)\n",
    "                return references_until_end.iloc[:end_of_references]\n",
    "            except IndexError as e:\n",
    "                print(e)\n",
    "                return references_until_end\n",
    "        except Exception as e:\n",
    "            return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/niklasgaertner/Desktop/coding/gpt_research/papers/comparison_between_constant_and_dynamic_vol_scaling.pdf \n",
      "\n",
      "['Risk adjusted momentum strategies: a comparisonbetween constant and dynamic volatility scalingapproaches', 'Abstract', '1. Introduction', '2. Data', '3. Methodology', '4. Empirical results', '5. Conclusion'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m references\u001b[39m.\u001b[39mappend(get_references(pdf_details))\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(chapters, \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m summary, tools \u001b[39m=\u001b[39m get_summaries(total_text, chapters)\n\u001b[1;32m     20\u001b[0m text, overview \u001b[39m=\u001b[39m get_overview(summary)\n\u001b[1;32m     21\u001b[0m overviews[path] \u001b[39m=\u001b[39m overview\n",
      "Cell \u001b[0;32mIn[152], line 13\u001b[0m, in \u001b[0;36mget_summaries\u001b[0;34m(text, chapters)\u001b[0m\n\u001b[1;32m     11\u001b[0m data[chapter] \u001b[39m=\u001b[39m [chapter_loc, next_chapter_loc]\n\u001b[1;32m     12\u001b[0m \u001b[39mtry\u001b[39;00m: \n\u001b[0;32m---> 13\u001b[0m     summary_by_chapter[chapter] \u001b[39m=\u001b[39m query_gpt(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mSummarise this chapter of a research paper called \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mchapter\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m: \u001b[39;49m\u001b[39m{\u001b[39;49;00mtext_by_chapter[chapter]\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     14\u001b[0m     \u001b[39m# statistical_tools[chapter] = query_gpt(f'What statistical tools are the authors using in this chapter? Return just a list of tools. No text, just list. Use \"*\" as bullet points: {text_by_chapter[chapter]}')\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidRequestError \u001b[39mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[151], line 5\u001b[0m, in \u001b[0;36mquery_gpt\u001b[0;34m(text, model, **kwargs)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mquery_gpt\u001b[39m(text, model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m----> 5\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      6\u001b[0m         messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m      7\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mYou are a helpful research assistant.\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m      8\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: text},\n\u001b[1;32m      9\u001b[0m         ],\n\u001b[1;32m     10\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     11\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/coding/gpt_research/gpt_venv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/Desktop/coding/gpt_research/gpt_venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/Desktop/coding/gpt_research/gpt_venv/lib/python3.10/site-packages/openai/api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[1;32m    291\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    292\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    293\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    294\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    295\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/Desktop/coding/gpt_research/gpt_venv/lib/python3.10/site-packages/openai/api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m     _thread_context\u001b[39m.\u001b[39msession_create_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    595\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    597\u001b[0m         method,\n\u001b[1;32m    598\u001b[0m         abs_url,\n\u001b[1;32m    599\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    600\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    601\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    602\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    603\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    604\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    607\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/coding/gpt_research/gpt_venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Desktop/coding/gpt_research/gpt_venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/Desktop/coding/gpt_research/gpt_venv/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/Desktop/coding/gpt_research/gpt_venv/lib/python3.10/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/coding/gpt_research/gpt_venv/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/Desktop/coding/gpt_research/gpt_venv/lib/python3.10/site-packages/urllib3/connection.py:454\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    453\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    456\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1375\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filepaths = []\n",
    "\n",
    "paths = [Path().cwd().joinpath('papers').joinpath(paper) for paper in os.listdir('papers')]\n",
    "\n",
    "pdf_detailss = []\n",
    "references = []\n",
    "overviews = {}\n",
    "texts = {}\n",
    "\n",
    "for path in paths[:2]:\n",
    "    try:\n",
    "        print(path, '\\n')\n",
    "        \n",
    "        chapters, total_text, pdf_details = get_text_toc(path)\n",
    "        pdf_detailss.append(pdf_details)\n",
    "        references.append(get_references(pdf_details))\n",
    "        print(chapters, '\\n')\n",
    "\n",
    "        summary, tools = get_summaries(total_text, chapters)\n",
    "        text, overview = get_overview(summary)\n",
    "        overviews[path] = overview\n",
    "        texts[path] = text\n",
    "        print(overview)\n",
    "        print('\\n\\n\\n')\n",
    "    except Exception as e:\n",
    "        print(path, '\\n')\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = references[-1]\n",
    "reference_text = ''\n",
    "for text in refs.text:\n",
    "    reference_text += text + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ang, A. and G. Bekaert (2007). Stock return predictability: Is it there?The Review of Financial Stud-\n",
      "ies 20(3), 651707.\n",
      "Ang, A., R. J. Hodrick, Y. Xing, and X. Zhang (2006). The cross-section of volatility and expected returns.\n",
      "The Journal of Finance 61(1), 259299.\n",
      "Arnott, R. D., M. Clements, V. Kalesnik, and J. T. Linnainmaa (2019). Factor momentum. Working paper.\n",
      "University of Southern California.\n",
      "Asness, C. S. (2016a). My factor philippic. Working paper. AQR Capital Management.\n",
      "Asness, C. S. (2016b).The siren song of factor timing.The Journal of Portfolio Management Special\n",
      "Issue(1).\n",
      "Asness, C. S., A. Frazzini, and L. H. Pedersen (2019). Quality minus junk. Review of Accounting Stud-\n",
      "ies 24(1), 34112.\n",
      "Banz, R. W. (1981).The relationship between return and market value of common stocks.Journal of\n",
      "Financial Economics 9(1), 318.\n",
      "Barr Rosenberg, K. R. and R. Lanstein (1984). Persuasive evidence of market ineciency. The Journal of\n",
      "Portfolio Management 11, 917.\n",
      "Barroso, P. and P. Santa-Clara (2015). Momentum has its moments. Journal of Financial Economics 116(1),\n",
      "111120.\n",
      "Basu, S. (1983). The relationship between earnings yield, market value and return for nyse common stocks:\n",
      "Further evidence. Journal of Financial Economics 12(1), 129156.\n",
      "Bender, J. and T. Wang (2016). Can the whole be more than the sum of the parts? bottom-up versus\n",
      "top-down multifactor portfolio construction. The Journal of Portfolio Management 42(5), 3950.\n",
      "Booth, D. G. and E. F. Fama (1992). Diversication returns and asset contributions. Financial Analysts\n",
      "Journal 48(3), 2632.\n",
      "Boudoukh, J., M. Richardson, and R. F. Whitelaw (2008). The myth of long-horizon predictability. The\n",
      "Review of Financial Studies 21(4), 15771605.\n",
      "20\n",
      "Brinson, G. P., L. R. Hood, and G. L. Beebower (1995). Determinants of portfolio performance. Financial\n",
      "Analysts Journal 51(1), 133138.\n",
      "Campbell, J. Y. and S. B. Thompson (2008). Predicting excess stock returns out of sample: Can anything\n",
      "beat the historical average? The Review of Financial Studies 21(4), 15091531.\n",
      "Chen, Y., W. Huang, and G. J. Jiang (2022). Do short-term institutions exploit stock return anomalies?\n",
      "The Financial Review 57(1), 6994.\n",
      "Clark, T. E. and K. D. West (2007). Approximately normal tests for equal predictive accuracy in nested\n",
      "models. Journal of Econometrics 138(1), 291311.\n",
      "Daniel, K., D. Hirshleifer, and L. Sun (2020). Short-and long-horizon behavioral factors. The Review of\n",
      "Financial Studies 33(4), 16731736.\n",
      "Daniel, K. and T. J. Moskowitz (2016).Momentum crashes.Journal of Financial Economics 122(2),\n",
      "221247.\n",
      "De Bondt, W. F. and R. Thaler (1985). Does the stock market overreact? The Journal of Finance 40(3),\n",
      "793805.\n",
      "Ehsani, S. and J. T. Linnainmaa (2021). Factor momentum and the momentum factor. The Journal of\n",
      "Finance, Forthcoming.\n",
      "Fama, E. F. and K. R. French (2012). Size, value, and momentum in international stock returns. Journal of\n",
      "Financial Economics 105(3), 457472.\n",
      "Fama, E. F. and K. R. French (2015).A ve-factor asset pricing model.Journal of Financial Eco-\n",
      "nomics 116(1), 122.\n",
      "Fan, M., F. Kearney, Y. Li, and J. Liu (2020). Momentum and the cross-section of stock volatility. Working\n",
      "paper. Queens University Belfast.\n",
      "Fan, M., Y. Li, and J. Liu (2018). Risk adjusted momentum strategies: a comparison between constant and\n",
      "dynamic volatility scaling approaches. Research in International Business and Finance.\n",
      "Frazzini, A. and L. H. Pedersen (2014). Betting against beta. Journal of Financial Economics 111(1), 125.\n",
      "21\n",
      "Guo, J., P. Li, and Y. Li (2021). What can explain momentum? evidence from decomposition. Management\n",
      "Science.\n",
      "Gupta, T. and B. Kelly (2019). Factor momentum everywhere. The Journal of Portfolio Management 45(3),\n",
      "1336.\n",
      "Han, X. (2022). Understanding the performance of components in betting against beta. Critical Finance\n",
      "Review 11(1), 136.\n",
      "Hansen, P. R. (2005). A test for superior predictive ability. Journal of Business & Economic Statistics 23(4),\n",
      "365380.\n",
      "Harvey, C. R. and Y. Liu (2020). False (and missed) discoveries in nancial economics. The Journal of\n",
      "Finance 75(5), 25032553.\n",
      "Hjalmarsson, E. (2010). Predicting global stock returns. Journal of Financial and Quantitative Analysis,\n",
      "4980.\n",
      "Hong, H., T. Lim, and J. C. Stein (2000). Bad news travels slowly: Size, analyst coverage, and the protability\n",
      "of momentum strategies. The Journal of Finance 55(1), 265295.\n",
      "Hou, K., H. Mo, C. Xue, and L. Zhang (2021). An augmented q-factor model with expected growth. Review\n",
      "of Finance 25(1), 141.\n",
      "Hou, K., C. Xue, and L. Zhang (2015). Digesting anomalies: An investment approach. The Review of\n",
      "Financial Studies 28(3), 650705.\n",
      "Hsu, Y.-C., C.-M. Kuan, and M.-F. Yen (2014). A generalized stepwise procedure with improved power for\n",
      "multiple inequalities testing. Journal of Financial Econometrics 12(4), 730755.\n",
      "Huang, D., J. Li, L. Wang, and G. Zhou (2020). Time series momentum: Is it there? Journal of Financial\n",
      "Economics 135(3), 774794.\n",
      "Jegadeesh, N. (1990). Evidence of predictable behavior of security returns. The Journal of Finance 45(3),\n",
      "881898.\n",
      "Jegadeesh, N. and S. Titman (1993). Returns to buying winners and selling losers: Implications for stock\n",
      "market eciency. The Journal of Finance 48(1), 6591.\n",
      "22\n",
      "Koijen, R. S., H. Lustig, and S. Van Nieuwerburgh (2017). The cross-section and time series of stock and\n",
      "bond returns. Journal of Monetary Economics 88, 5069.\n",
      "Li, J. and J. Yu (2012). Investor attention, psychological anchors, and stock return predictability. Journal\n",
      "of Financial Economics 104(2), 401419.\n",
      "Liu, J. and F. Papailias (2021).Time series reversal in trend-following strategies.European Financial\n",
      "Management.\n",
      "Loughran, T. and J. R. Ritter (1995). The new issues puzzle. The Journal of Finance 50(1), 2351.\n",
      "Moskowitz, T. J. and M. Grinblatt (1999). Do industries explain momentum? The Journal of Finance 54(4),\n",
      "12491290.\n",
      "Moskowitz, T. J., Y. H. Ooi, and L. H. Pedersen (2012). Time series momentum. Journal of Financial\n",
      "Economics 104(2), 228250.\n",
      "Newey, W. K. and K. D. West (1987). A simple, positive semi-denite, heteroskedasticity and autocorrelation\n",
      "consistent covariance matrix. Econometrica 55(3), 703708.\n",
      "Novy-Marx, R. (2013). The other side of value: The gross protability premium. Journal of Financial\n",
      "Economics 108(1), 128.\n",
      "Papailias, F., J. Liu, and D. D. Thomakos (2021).Return signal momentum.Journal of Banking &\n",
      "Finance 124, 106063.\n",
      "Pastor, L. and R. F. Stambaugh (2003). Liquidity risk and expected stock returns. Journal of Political\n",
      "Economy 111(3), 642685.\n",
      "Romano, J. P., A. M. Shaikh, and M. Wolf (2008). Formalized data snooping based on generalized error\n",
      "rates. Econometric Theory, 404447.\n",
      "Rouwenhorst, K. G. (1998). International momentum strategies. The Journal of Finance 53(1), 267284.\n",
      "Sloan, R. G. (1996). Do stock prices fully reect information in accruals and cash ows about future earnings?\n",
      "The Accounting Review, 289315.\n",
      "Stambaugh, R. F., J. Yu, and Y. Yuan (2012). The short of it: Investor sentiment and anomalies. Journal\n",
      "of Financial Economics 104(2), 288302.\n",
      "23\n",
      "Sullivan, T. G. (1978). The cost of capital and the market power of rms. The Review of Economics and\n",
      "Statistics, 209217.\n",
      "Titman, S., K. J. Wei, and F. Xie (2004). Capital investments and stock returns. Journal of Financial and\n",
      "Quantitative Analysis 39(4), 677700.\n",
      "Van Gelderen, E., J. Huij, and G. Kyosev (2019). Factor investing from concept to implementation. The\n",
      "Journal of Portfolio Management 45(3), 125140.\n",
      "White, H. (2000). A reality check for data snooping. Econometrica 68(5), 10971126.\n",
      "24\n",
      "Table 1: Summary statistics of EL 22 factors.\n",
      "Summary statistics of EL 22 factors. The rst three columns report the factor names, abbreviations, and original studies. The fourth column reports the samplestart dates, and the end date for all factors is July 2015. Columns Five to Eight report the annualised mean returns, standard deviations, Sharpe ratios, and rstorder autocorrelations. *, **, and *** indicate that the coecients are statistically signicant at the 10%, 5% and 1% levels, respectively.\n",
      "FactorAbbrevOriginal StudyStart dateMean (%)SD (%)SRAR(1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(reference_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The author is investigating the issue of risk-adjusted momentum strategies and specifically comparing the performance of constant and dynamic volatility scaling approaches.\\n\\nThe author is investigating the performance of two volatility scaling methods in momentum strategies, specifically comparing the constant volatility scaling approach of Barroso and Santa-Clara (2015) to the dynamic volatility scaling method of Daniel and Moskowitz (2016).\\n\\nThe author is investigating the issue of momentum crashes in momentum strategies. Specifically, they are examining the implementation of volatility scaling methods to address the risk of momentum crashes in futures markets across different asset classes.\\n\\nThe author is investigating the properties and characteristics of various global liquid futures instruments, including commodities, sovereign bonds, currencies, and equity index contracts, along with the returns and volatility associated with these asset classes.\\n\\nThe author is investigating the methodology and performance of cross-section momentum (XSMOM) strategies in financial markets.\\n\\nThe author is investigating the issue of momentum crashes in futures markets and the effect of volatility scaling approaches in reducing these losses during times of financial stress.'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "answers = []\n",
    "for i in range(0,len(chapters)):\n",
    "    context = total_text[total_text.find(chapters[i-1]):total_text.find(chapters[i])]\n",
    "\n",
    "    \n",
    "    answers.append(query_gpt_with_context_detailed(text=question, context=context[:4000]))\n",
    "context = '\\n'.join(answers)\n",
    "query_gpt_with_context_detailed(text='summarise the relevant parts of the context', context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15030"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = total_text[total_text.find(chapters[i-1]):total_text.find(chapters[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The empirical results in this research study focus on momentum crashes in futures markets and the comparison between constant and dynamic volatility scaling approaches. Momentum crashes occur when the cumulative returns of the bottom decile are significantly higher than the top decile, typically during times of financial stress. The study identifies momentum crashes in futures markets during the 2007-2008 financial crisis.\\n\\nThe comparison between constant volatility scaling (CVS) and dynamic volatility scaling (DVS) approaches shows that CVS outperforms DVS in terms of alpha (excess return) during the entire sample period. However, during the financial crisis period, the performance of both CVS and DVS is negatively impacted and the difference in alphas between the two approaches becomes insignificant. The study also compares the scaled XSMOM (Cross-sectional Momentum) strategies with benchmarks such as buy-and-hold and time series momentum (TSMOM) strategies. The scaled XSMOM strategies outperform the benchmarks in terms of alpha.\\n\\nOverall, the findings suggest that constant volatility scaling is more efficient than dynamic volatility scaling, but the performance advantage of CVS over DVS is greatly reduced during times of financial crisis. The scaled XSMOM strategies also outperform the benchmarks, providing further evidence of the effectiveness of volatility scaling in momentum strategies.'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_gpt(f'Summarise this:  {context[:12000]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    query_gpt_with_context_detailed('summarise the context', context)\n",
    "except InvalidRequestError as e:\n",
    "    a = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = current_length / max_length\n",
    "\n",
    "if ratio % 1 < 0.8:\n",
    "    num_parts = int(ratio) + 1\n",
    "else:\n",
    "    num_parts = int(ratio) + 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0973883329265317"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio % 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0973883329265317"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = current_length / max_length\n",
    "if ratio < 1.5:\n",
    "    #split string in half \n",
    "elif ratio > 2:\n",
    "    #split current string by ratio \n",
    "else:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The context is focused on empirical results related to momentum crashes in futures markets. The study examines the performance of constant and dynamic volatility scaling approaches in reducing momentum losses during times of financial stress. The analysis includes the identification of momentum crashes caused by the 2007-2008 financial crisis, as well as a comparison between the two scaling approaches. Overall, the constant volatility scaling approach outperforms the dynamic volatility scaling approach, but this superiority is diminished during the financial crisis period.'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_gpt_with_context_concise('summarise the context', context[:12000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The context is focused on the empirical results of momentum crashes in futures markets and the comparison between constant volatility scaling (CVS) and dynamic volatility scaling (DVS) approaches. Momentum crashes are periods when the cumulative returns of the bottom decile are significantly higher than the cumulative returns of the top decile. The study identifies a momentum crash caused by the 2007-2008 financial crisis in futures markets. The comparison between CVS and DVS approaches reveals that CVS is more efficient overall, but the superiority is almost eliminated during times of financial crisis. The study also includes a cross-strategy comparison, considering buy-and-hold, time series momentum (TSMOM), and cross-sectional momentum (XSMOM) strategies. The scaled XSMOM strategies outperform the benchmark strategies.'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_gpt_with_context('summarise the context', context[:12000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cc2234cb638fd13d2f887c1bf165d025dd1855b699db760ed5ba25b9bbfc1a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
